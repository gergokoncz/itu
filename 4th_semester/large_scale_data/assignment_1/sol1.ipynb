{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Scale Data Analysis - Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Implement image classifier with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train and test set and separate labels\n",
    "train_all = np.load('fashion_train.npy')\n",
    "test_all = np.load('fashion_test.npy')\n",
    "\n",
    "train_labels = train_all[:,-1]\n",
    "train_featues = train_all[:, :-1]\n",
    "\n",
    "test_labels = test_all[:,-1]\n",
    "test_features = test_all[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn elements for preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# for classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# for validation and evaluation\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before classification I have used the combination of standard scaling and Principal Component Analysis to normalize the data to some degree and also to keep only a smaller number of features instead of the original 28 * 28 for the each picture. I have experimented with different number of components received from PCA, and I have experienced an increase in performance until I have increased it to 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up pipeline and process train data\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\", PCA(n_components=50))\n",
    "])\n",
    "\n",
    "processed_train_data = preprocessing_pipeline.fit_transform(train_featues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have trained 4 classifiers (SVM, RandomForest, DecisionTree, LogisticRegression) and also examined their performance after changing their parameters. I have compared their accurary score from 5-fold cross-validations. SVM with polykernel and RandomForest with maximum tree depth of 10 seemed like the best candidates. By SVC I have also tried to a Gaussian kernel, but it was outperformed by polykernel with the chosen parameters. LogisticRegression worked much better when I have applied the penalty term 'l2'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.8215, 0.818 , 0.8195, 0.825 , 0.819 ])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LogisticRegression with cross validation\n",
    "lr_classifier = LogisticRegression(penalty='l2', tol=0.00001)\n",
    "cross_val_score(lr_classifier, processed_train_data, train_labels, cv = 5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.8385, 0.832 , 0.839 , 0.841 , 0.8395])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RandomForestClassifier with cross validation\n",
    "rf_classifier = RandomForestClassifier(max_depth = 10)\n",
    "cross_val_score(rf_classifier, processed_train_data, train_labels, cv = 5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.764 , 0.773 , 0.7635, 0.77  , 0.7675])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decisionTreeClassifier with cross calidation\n",
    "dc_classifier = DecisionTreeClassifier(max_depth=10)\n",
    "cross_val_score(dc_classifier, processed_train_data, train_labels, cv = 5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.8675, 0.8535, 0.864 , 0.8535, 0.8535])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM classiffier with cross validation\n",
    "sv_classifier = SVC(kernel='poly', degree = 3, coef0=2, C = 5)\n",
    "cross_val_score(sv_classifier, processed_train_data, train_labels, cv = 5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process testing data\n",
    "processed_test_data = preprocessing_pipeline.transform(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for final evaluation I have trained the classifiers with the picked parameters on the whole training data to test their accuracy, macro precision, macro recall and confusion matrix.\n",
    "The best results were produced by SVM classifier with very high performance on class 1 and class 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "acc = 0.8122\nprecision = 0.8107132210685485\nrecall = 0.8122\n[[810   2  12  64 112]\n [  5 953  10  27   5]\n [ 23   4 808  18 147]\n [ 32  17   7 881  63]\n [169   5 164  53 609]]\n"
    }
   ],
   "source": [
    "lr_classifier.fit(processed_train_data, train_labels)\n",
    "lr_test_y = lr_classifier.predict(processed_test_data)\n",
    "print(f\"acc = {accuracy_score(test_labels, lr_test_y)}\")\n",
    "print(f\"precision = {precision_score(test_labels, lr_test_y, average= 'macro')}\")\n",
    "print(f\"recall = {recall_score(test_labels, lr_test_y, average = 'macro')}\")\n",
    "print(confusion_matrix(test_labels, lr_test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "acc = 0.845\nprecision = 0.8440280874600662\nrecall = 0.845\n[[824   6  17  38 115]\n [  4 966   4  20   6]\n [ 30   4 845  23  98]\n [ 35   9  13 910  33]\n [155   5 122  38 680]]\n"
    }
   ],
   "source": [
    "# \n",
    "sv_classifier.fit(processed_train_data, train_labels)\n",
    "sv_test_y = sv_classifier.predict(processed_test_data)\n",
    "print(f\"acc = {accuracy_score(test_labels, sv_test_y)}\")\n",
    "print(f\"precision = {precision_score(test_labels, sv_test_y, average= 'macro')}\")\n",
    "print(f\"recall = {recall_score(test_labels, sv_test_y, average = 'macro')}\")\n",
    "print(confusion_matrix(test_labels, sv_test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "acc = 0.8222\nprecision = 0.8207599478550769\nrecall = 0.8221999999999999\n[[810   2  15  63 110]\n [  7 936   9  41   7]\n [ 18   0 849  20 113]\n [ 28  11   5 912  44]\n [186   1 163  46 604]]\n"
    }
   ],
   "source": [
    "rf_classifier.fit(processed_train_data, train_labels)\n",
    "rf_test_y = rf_classifier.predict(processed_test_data)\n",
    "print(f\"acc = {accuracy_score(test_labels, rf_test_y)}\")\n",
    "print(f\"precision = {precision_score(test_labels, rf_test_y, average= 'macro')}\")\n",
    "print(f\"recall = {recall_score(test_labels, rf_test_y, average = 'macro')}\")\n",
    "print(confusion_matrix(test_labels, rf_test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "acc = 0.771\nprecision = 0.7744361861506388\nrecall = 0.771\n[[722   9  21  88 160]\n [ 11 906  17  46  20]\n [ 24   7 751  37 181]\n [ 30  14  19 863  74]\n [160   8 164  55 613]]\n"
    }
   ],
   "source": [
    "dc_classifier.fit(processed_train_data, train_labels)\n",
    "dc_test_y = dc_classifier.predict(processed_test_data)\n",
    "print(f\"acc = {accuracy_score(test_labels, dc_test_y)}\")\n",
    "print(f\"precision = {precision_score(test_labels, dc_test_y, average= 'macro')}\")\n",
    "print(f\"recall = {recall_score(test_labels, dc_test_y, average = 'macro')}\")\n",
    "print(confusion_matrix(test_labels, dc_test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Keras for image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task I have implemented a simple network with two dense layers (with added dropout layers) plus a softmax at the end. I have experienced the best results with a using ReLU as activation function for both dense layers, since I have got lesser results with using hyperbolic tangent. \n",
    "To avoid overfitting I have chosen to use dropout, without it the net was likely to overfit after epoch 20. \n",
    "The reason I have not gone to a more advanced network (CNN) is that I wanted to see the power of a simple network and even this network outperformed the best ML classifier with 86% accuracy over 84% for poly-kernel SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing input, scaling 0-1\n",
    "train_featues_scaled = train_featues / 255.0\n",
    "test_features_scaled = test_features / 255.0\n",
    "# separate development set\n",
    "train_featues_scaled_train = train_featues_scaled[:7000, :]\n",
    "train_labels_train = train_labels[:7000]\n",
    "\n",
    "train_labels_dev = train_labels[7000:]\n",
    "train_featues_scaled_dev = train_featues_scaled[7000:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(300, activation = \"relu\", input_shape = [784,]),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(100, activation = \"relu\"),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(5, activation = \"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 300)               235500    \n_________________________________________________________________\ndropout (Dropout)            (None, 300)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 100)               30100     \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 100)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 5)                 505       \n=================================================================\nTotal params: 266,105\nTrainable params: 266,105\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = keras.optimizers.SGD(lr = 0.01), metrics = [\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_featues_scaled_train, train_labels_train, epochs = 30, validation_data=(train_featues_scaled_dev, train_labels_dev), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_featues_scaled, train_labels, epochs = 30, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = model.evaluate(test_features_scaled, test_labels, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[0.3916806790351868, 0.8588]\n"
    }
   ],
   "source": [
    "print(acc)"
   ]
  }
 ]
}