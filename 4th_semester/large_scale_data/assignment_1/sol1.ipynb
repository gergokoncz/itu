{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Scale Data Analysis\n",
    "## Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Implement image classifier with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train and test set and separate labels\n",
    "train_all = np.load('fashion_train.npy')\n",
    "test_all = np.load('fashion_test.npy')\n",
    "\n",
    "train_labels = train_all[:,-1]\n",
    "train_featues = train_all[:, :-1]\n",
    "\n",
    "test_labels = test_all[:,-1]\n",
    "test_features = test_all[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn elements for preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# for classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# for validation and evaluation\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before classification I have used the combination of standard scaling and Principal Component Analysis to normalize the data to some degree and also to keep only a smaller number of features instead of the original 28 * 28 for the each picture. I have experimented with different number of components received from PCA, and I have experienced an increase in performance until I have increased it to 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up pipeline and process train data\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\", PCA(n_components=50))\n",
    "])\n",
    "\n",
    "processed_train_data = preprocessing_pipeline.fit_transform(train_featues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have trained 4 classifiers (SVM, RandomForest, DecisionTree, LogisticRegression) and also examined their performance after changing their parameters. I have compared their accurary score from 5-fold cross-validations. SVM with polykernel and RandomForest with 10 maximum tree depth seemed like the best candidates. By SVC I have also tried to a Gaussian kernel, but it was outperformed by polykernel with the chosen parameters. LogisticRegression worked much better when I have applied the penalty term 'l2'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.82  , 0.8135, 0.82  , 0.822 , 0.82  ])"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LogisticRegression with cross validation\n",
    "lr_classifier = LogisticRegression(penalty='l2', tol=0.00001)\n",
    "cross_val_score(lr_classifier, processed_train_data, train_labels, cv = 5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.8375, 0.8325, 0.8395, 0.838 , 0.828 ])"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RandomForestClassifier with cross validation\n",
    "rf_classifier = RandomForestClassifier(max_depth = 10)\n",
    "cross_val_score(rf_classifier, processed_train_data, train_labels, cv = 5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.762 , 0.776 , 0.7695, 0.7725, 0.766 ])"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decisionTreeClassifier with cross calidation\n",
    "dc_classifier = DecisionTreeClassifier(max_depth=10)\n",
    "cross_val_score(dc_classifier, processed_train_data, train_labels, cv = 5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.8635, 0.853 , 0.867 , 0.8505, 0.8515])"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM classiffier with cross validation\n",
    "sv_classifier = SVC(kernel='poly', degree = 3, coef0=2, C = 5)\n",
    "cross_val_score(sv_classifier, processed_train_data, train_labels, cv = 5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process testing data\n",
    "processed_test_data = preprocessing_pipeline.transform(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for final evaluation I have trained the classifiers with the picked parameters on the whole training data to test their accuracy, macro precision and macro recall and confusion matrix.\n",
    "Best results were produced by SVM classifier with very high performance on class 1 and class 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "acc = 0.8122\nprecision = 0.8104489697732291\nrecall = 0.8122\n[[810   3  12  63 112]\n [  5 953  10  27   5]\n [ 23   4 811  18 144]\n [ 32  16   8 882  62]\n [170   6 165  54 605]]\n"
    }
   ],
   "source": [
    "lr_classifier.fit(processed_train_data, train_labels)\n",
    "lr_test_y = lr_classifier.predict(processed_test_data)\n",
    "print(f\"acc = {accuracy_score(test_labels, lr_test_y)}\")\n",
    "print(f\"precision = {precision_score(test_labels, lr_test_y, average= 'macro')}\")\n",
    "print(f\"recall = {recall_score(test_labels, lr_test_y, average = 'macro')}\")\n",
    "print(confusion_matrix(test_labels, lr_test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "acc = 0.8448\nprecision = 0.8438225297590956\nrecall = 0.8448\n[[820   6  14  35 125]\n [  5 966   2  22   5]\n [ 32   4 847  22  95]\n [ 35   7  10 915  33]\n [157   7 123  37 676]]\n"
    }
   ],
   "source": [
    "# \n",
    "sv_classifier.fit(processed_train_data, train_labels)\n",
    "sv_test_y = sv_classifier.predict(processed_test_data)\n",
    "print(f\"acc = {accuracy_score(test_labels, sv_test_y)}\")\n",
    "print(f\"precision = {precision_score(test_labels, sv_test_y, average= 'macro')}\")\n",
    "print(f\"recall = {recall_score(test_labels, sv_test_y, average = 'macro')}\")\n",
    "print(confusion_matrix(test_labels, sv_test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "acc = 0.8278\nprecision = 0.8267267677707185\nrecall = 0.8278000000000001\n[[813   1  13  67 106]\n [  3 936   8  45   8]\n [ 15   0 861  23 101]\n [ 30   9   6 913  42]\n [177   0 160  47 616]]\n"
    }
   ],
   "source": [
    "rf_classifier.fit(processed_train_data, train_labels)\n",
    "rf_test_y = rf_classifier.predict(processed_test_data)\n",
    "print(f\"acc = {accuracy_score(test_labels, rf_test_y)}\")\n",
    "print(f\"precision = {precision_score(test_labels, rf_test_y, average= 'macro')}\")\n",
    "print(f\"recall = {recall_score(test_labels, rf_test_y, average = 'macro')}\")\n",
    "print(confusion_matrix(test_labels, rf_test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "acc = 0.7684\nprecision = 0.7702856724341037\nrecall = 0.7684\n[[725   5  26  67 177]\n [ 12 906  16  48  18]\n [ 31   6 756  42 165]\n [ 37  29  21 853  60]\n [164  10 159  65 602]]\n"
    }
   ],
   "source": [
    "dc_classifier.fit(processed_train_data, train_labels)\n",
    "dc_test_y = dc_classifier.predict(processed_test_data)\n",
    "print(f\"acc = {accuracy_score(test_labels, dc_test_y)}\")\n",
    "print(f\"precision = {precision_score(test_labels, dc_test_y, average= 'macro')}\")\n",
    "print(f\"recall = {recall_score(test_labels, dc_test_y, average = 'macro')}\")\n",
    "print(confusion_matrix(test_labels, dc_test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Keras for image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task I have implemented a simple network with two dense layers (with added dropout layers) plus a softmax at the end. I have experienced the best results with a using ReLU as activation function for both dense layers, since I have got lesser results with using hyperbolic tangent. \n",
    "To avoid overfitting I have chosen to use dropout, without it the net was likely to overfit after epoch 20. \n",
    "The reason I have not gone to a more advanced network (CNN) is that currently I am still in the process of deepening my keras knowledge and even this network outperformed the best ML classifier with 86% accuracy over 84% for poly-kernel SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing input, scaling 0-1\n",
    "train_featues_scaled = train_featues / 255.0\n",
    "test_features_scaled = test_features / 255.0\n",
    "# separate development set\n",
    "train_featues_scaled_train = train_featues_scaled[:7000, :]\n",
    "train_labels_train = train_labels[:7000]\n",
    "\n",
    "train_labels_dev = train_labels[7000:]\n",
    "train_featues_scaled_dev = train_featues_scaled[7000:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(300, activation = \"relu\", input_shape = [784,]),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(100, activation = \"relu\"),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(5, activation = \"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"sequential_12\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_36 (Dense)             (None, 300)               235500    \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 300)               0         \n_________________________________________________________________\ndense_37 (Dense)             (None, 100)               30100     \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 100)               0         \n_________________________________________________________________\ndense_38 (Dense)             (None, 5)                 505       \n=================================================================\nTotal params: 266,105\nTrainable params: 266,105\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = keras.optimizers.SGD(lr = 0.01), metrics = [\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_featues_scaled_train, train_labels_train, epochs = 30, validation_data=(train_featues_scaled_dev, train_labels_dev), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_featues_scaled, train_labels, epochs = 30, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = model.evaluate(test_features_scaled, test_labels, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}