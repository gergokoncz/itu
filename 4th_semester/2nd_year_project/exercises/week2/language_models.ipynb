{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model Exercises\n",
    "\n",
    "\n",
    "In this exercise, you will build a system for automatically generating sentences using an n-gram language model.\n",
    "\n",
    "You will:\n",
    "- Build a unigram language model \n",
    "- Evaluate your unigram language model\n",
    "- Build a bigram language model and compare it to the unigram model (generation and perplexity)\n",
    "\n",
    "\n",
    "Key concepts:\n",
    "\n",
    "* n-gram (unigram, bigram, trigram, etc.)\n",
    "* n-gram history\n",
    "* n-gram probability \n",
    "* intrinsic LM evaluation - perplexity\n",
    "* OOV words and how to handle them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---\n",
    "Latex Macros\n",
    "-->\n",
    "$$\n",
    "\\newcommand{\\Xs}{\\mathcal{X}}\n",
    "\\newcommand{\\Ys}{\\mathcal{Y}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\weights}{\\mathbf{w}}\n",
    "\\newcommand{\\balpha}{\\boldsymbol{\\alpha}}\n",
    "\\newcommand{\\bbeta}{\\boldsymbol{\\beta}}\n",
    "\\newcommand{\\aligns}{\\mathbf{a}}\n",
    "\\newcommand{\\align}{a}\n",
    "\\newcommand{\\source}{\\mathbf{s}}\n",
    "\\newcommand{\\target}{\\mathbf{t}}\n",
    "\\newcommand{\\ssource}{s}\n",
    "\\newcommand{\\starget}{t}\n",
    "\\newcommand{\\repr}{\\mathbf{f}}\n",
    "\\newcommand{\\repry}{\\mathbf{g}}\n",
    "\\newcommand{\\bar}{\\,|\\,}\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\prob}{p}\n",
    "\\newcommand{\\Pulp}{\\text{Pulp}}\n",
    "\\newcommand{\\Fiction}{\\text{Fiction}}\n",
    "\\newcommand{\\PulpFiction}{\\text{Pulp Fiction}}\n",
    "\\newcommand{\\pnb}{\\prob^{\\text{NB}}}\n",
    "\\newcommand{\\vocab}{V}\n",
    "\\newcommand{\\params}{\\boldsymbol{\\theta}}\n",
    "\\newcommand{\\param}{\\theta}\n",
    "\\DeclareMathOperator{\\perplexity}{PP}\n",
    "\\DeclareMathOperator{\\argmax}{argmax}\n",
    "\\DeclareMathOperator{\\argmin}{argmin}\n",
    "\\newcommand{\\train}{\\mathcal{D}}\n",
    "\\newcommand{\\counts}[2]{\\#_{#1}(#2) }\n",
    "\\newcommand{\\length}[1]{\\text{length}(#1) }\n",
    "\\newcommand{\\indi}{\\mathbb{I}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>Step 0</font>: Setup\n",
    "\n",
    "In order to develop this assignment, you will need at least [python 3.6](https://www.python.org/downloads/) and the following libraries. Most if not all of these are part of [anaconda](https://www.continuum.io/downloads), so a good starting point is to install that on your laptop. For this exercise you can use your laptop.\n",
    "\n",
    "- [jupyter](http://jupyter.readthedocs.org/en/latest/install.html)\n",
    "- numpy \n",
    "- [nosetests](https://nose.readthedocs.org/en/latest/) which is a library for unit testing \n",
    "- [pandas](http://pandas.pydata.org/) Dataframes\n",
    "\n",
    "Here is some help on installing packages in python: https://packaging.python.org/installing/. You can use ```pip --user``` to install locally without sudo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>Setup 1</font>: Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "My Python version\npython: 3.6.9 (default, Nov  7 2019, 10:44:02) \n[GCC 8.3.0]\n"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import sys\n",
    "import nose\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "print('My Python version')\n",
    "\n",
    "print('python: {}'.format(sys.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most of your coding will be in the python source files in the directory ```snlp```.\n",
    "- The directory ```tests``` contains unit tests that you can use to check parts of your assignment, using ```nosetests```. You should run them as you work on the assignment to see that you're on the right track. You are free to look at their source code, if that helps -- though most of the relevant code is also here in this notebook. Learn more about running unit tests at http://pythontesting.net/framework/nose/nose-introduction/\n",
    "- You may want to add more tests, but that is completely optional. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "My library versions\npandas: 1.0.0\nnumpy: 1.18.1\nnose: 1.3.7\n"
    }
   ],
   "source": [
    "print('My library versions')\n",
    "\n",
    "print('pandas: {}'.format(pd.__version__))\n",
    "print('numpy: {}'.format(np.__version__))\n",
    "print('nose: {}'.format(nose.__version__))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following command on the command line, to test whether your libraries are set up correctly:\n",
    "\n",
    "`nosetests tests/test_environment.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running command-line UNIX commands in the notebook\n",
    "\n",
    "You can prefix a cell in a notebook with `!` to tell the notebook to run what follows as shell command. For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nosetests tests/test_environment.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you setup your system successfully, you should see an output like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "nose.config: INFO: Ignoring files matching ['^\\\\.', '^_', '^setup\\\\.py$']\n",
    "test_environment.test_library_versions ... ok\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "Ran 1 test in 0.001s\n",
    "\n",
    "OK\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  <font color='blue'>Task 1</font>: Load the Data, Preprocessing (Tokenization)\n",
    "\n",
    "We will first read the data. We will work here with a tiny toy dataset. Here are two ways to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-21T16:59:18.613748",
     "start_time": "2016-10-21T16:59:18.575886"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>the dog barks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>this is a test sentence .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>another test sentence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>wow this is a nice idea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>this is a dog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   sentences\n",
       "0              the dog barks\n",
       "1  this is a test sentence .\n",
       "2      another test sentence\n",
       "3    wow this is a nice idea\n",
       "4              this is a dog"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using Pandas to read the csv\n",
    "df_train = pd.read_csv('data/corpus.csv')\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a unix command\n",
    "! cat data/corpus.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does the following command do?\n",
    "! cat data/corpus.csv | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(df_train)==7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple tokenizer\n",
    "Your first task is to convert the text into a representation which is internally used. For this data, a lot of the preprocessing is already done: the text is lower-cased, and punctuation is removed. You need to only create a `list` of words for each instance. Each word is tokenized using space.\n",
    "\n",
    "This first part makes you also familiar with the structure of the assignment, i.e., modifying the code in the `snlp` directory, loading it in the jupyter notebook and testing your solution with `nosetests`. If you first run the code below, you will get an `NotImplementedError`.\n",
    "\n",
    "- **Deliverable 1.1**: Complete the function `preproc.space_tokenizer`. \n",
    "- **Test**: `nosetest tests/test_preproc.py:test_space_tok`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snlp import preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this block to update the notebook as you change the preproc library\n",
    "reload(preproc);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = preproc.read_data('data/corpus.csv',preprocessor=preproc.space_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use ! to run shell commands in notebook\n",
    "! nosetests tests/test_preproc.py:test_space_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the vocabulary\n",
    "Your second task is to extract the vocabulary from the tokenized text.\n",
    "\n",
    "- **Deliverable 1.2**: Complete the function `preproc.create_vocab`. \n",
    "- **Test**: `nosetest tests/test_preproc.py:test_create_vocab`\n",
    "\n",
    "Hint: It might be helpful to check out the test code to get a feeling of what you are asked to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(preproc);\n",
    "# use ! to run shell commands in notebook\n",
    "! nosetests tests/test_preproc.py:test_create_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually inspecting the vocabulary\n",
    "print(preproc.create_vocab(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Task 2</font>: Create a uniform n-gram LM (unigram) and evaluate it\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a uniform language model. Before solving this task, make sure you have read all up to Section 3.1 of [J&M Chapter 3](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
    "\n",
    "Language models in this book implement the `LanguageModel` [abstract base class](https://docs.python.org/3/library/abc.html). See the definition of `class LanguageModel(metaclass=abc.ABCMeta)` in `snlp/lm.py`, copied below for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snlp import lm\n",
    "import abc\n",
    "\n",
    "class LanguageModel(metaclass=abc.ABCMeta):\n",
    "    \"\"\"\n",
    "    Abstract class for a language model\n",
    "    Args:\n",
    "        docs: the texts. Should be a list of documents (list of words).\n",
    "        order: history length (-1).\n",
    "    Creates:\n",
    "        vocab: the vocabulary underlying this language model. Should be a set of words.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab, order):\n",
    "        ## start and end symbols\n",
    "        self.vocab = vocab\n",
    "        self.order = order\n",
    "        self.START_SYMBOL = \"<START>\"\n",
    "        self.STOP_SYMBOL = \"<STOP>\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def probability(self, word, *history):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            word: the word we need the probability of\n",
    "            history: words to condition on.\n",
    "\n",
    "        Returns:\n",
    "            the probability p(w|history)\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important method we have to provide is `probability(word,history)` which returns the probability of a word given a history. Let us implement a uniform LM using this class. For that case, the history is simply empty `()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(lm);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a uniform LM\n",
    "\n",
    "- **Deliverable 2.1**: Complete the function `probability` in `lm.UniformLM`. It should return the probability of a word under a (stupid) uniform language model.\n",
    "- **Test**: `nosetest tests/test_lms.py:test_uniform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(lm);\n",
    "! nosetests tests/test_lms.py:test_uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling from the Uniform LM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have completed the code for the first and simplest LM, let's use it and sample from it to create example sentences and get an intuitive feeling on how good it is.\n",
    "\n",
    "\n",
    "- **Deliverable 2.2** (in the notebook here):  Create a `uniformLM` object. Inspect the object, how big is the vocabulary? What is the probability of the word `the`?  What is the probability of the word `dog`?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO:\n",
    "from snlp import lm\n",
    "x_train = preproc.read_data('data/corpus.csv',preprocessor=preproc.space_tokenizer)\n",
    "\n",
    "# instantiate a uniform LM \n",
    "vocab = pass\n",
    "uniformLM = pass\n",
    "\n",
    "# get size of vocab and probabilities of words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Deliverable 2.3** (in the notebook here): Use the uniform language model we just created and sample from it. What is an issue with this language model? Discuss disadvantages of a uniform LM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now sample a sentence from the uniform LM\n",
    "sentence_len = 10\n",
    "\n",
    "' '.join(sample(uniformLM, [\"the\"],sentence_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Read [Section 3.2. of J&M](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
    "\n",
    "How do we determine the quality of an (n-gram) LM? \n",
    "\n",
    "One way is through *extrinsic* evaluation: assess how much the LM improves performance on *downstream tasks* such as machine translation or speech recognition. Arguably this is the most important measure of LM quality, but it can be costly as re-training such systems may take days, and when we seek to develop general-purpose LMs we may have to evaluate performance on several tasks. This is problematic when one wants to iteratively improve LMs and test new models and parameters. It is hence useful to find *intrinsic* means of evaluation that assess the stand-alone quality of LMs with minimal overhead.\n",
    "\n",
    "An intrinsic way to measure the quality of a language model is to use a held out set to measure the performance of the language model trained on the training corpus, by evaluating it on the held out set. But what performance measure can we use, or which n-gram language model is a better LM? \n",
    "\n",
    "\"The answer is simple: whichever\n",
    "model assigns a higher probability to the test set—meaning it more accurately\n",
    "predicts the test set—is a better model. Given two probabilistic models, the better\n",
    "model is the one that has a tighter fit to the test data or that better predicts the details\n",
    "of the test data, and hence will assign a higher probability to the test data.\" (J&M)\n",
    "\n",
    "In practice we don’t use raw probability as our metric for evaluating language model perplexity, but a variant called **perplexity**. The perplexity (sometimes called PP for short) of a language model on a held out set is the inverse probability of the held ou set, normalized by the number of words. In more details:\n",
    "\n",
    "Given a test sequence \\\\(w_1,\\ldots,w_T\\\\) of \\\\(T\\\\) words, we calculate the perplexity \\\\(\\perplexity\\\\) as follows:\n",
    "\n",
    "$$\n",
    "\\perplexity(w_1,\\ldots,w_T) = \\prob(w_1,\\ldots,w_T)^{-\\frac{1}{T}} = \\sqrt[T]{\\prod_i^T \\frac{1}{\\prob(w_i|w_{i-n},\\ldots,w_{i-1})}}\n",
    "$$\n",
    "\n",
    "We can implement a perplexity function based on the `LanguageModel` interface. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-13T11:56:45.059308",
     "start_time": "2016-10-13T11:56:45.042382"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def perplexity(lm, data):\n",
    "    \"\"\"\n",
    "    Calculate the perplexity of the language model given the provided data.\n",
    "    Args:\n",
    "        lm: a language model.\n",
    "        data: the data to calculate perplexity on.\n",
    "\n",
    "    Returns:}\n",
    "        the perplexity of `lm` on `data`.\n",
    "\n",
    "    \"\"\"\n",
    "    log_prob = 0.0\n",
    "    history_order = lm.order - 1\n",
    "    \n",
    "    # flatten data\n",
    "    sdata = [word for sentence in data for word in  sentence] \n",
    "    for i in range(history_order, len(sdata)):\n",
    "        history = sdata[i - history_order : i]\n",
    "        word = sdata[i]\n",
    "        p = lm.probability(word, *history)\n",
    "        log_prob += math.log(p) if p > 0.0 else float(\"-inf\")\n",
    "    return math.exp(-log_prob / (len(sdata) - history_order))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the uniform model does on our held-out dataset. \n",
    "\n",
    "- **Deliverable 2.4** (in the notebook here): Calculate the perplexity of the uniform LM on the dev data. First, inspect the `corpus_dev.txt` file and manually calculate the perplexity. Explain the perplexity that you get. Then, use the function above and evaluate perplexity  on the data file `corpus_dev.txt`. Does it match your expectation? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-13T11:56:45.098765",
     "start_time": "2016-10-13T11:56:45.060841"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "### read dev data and calculate uniformLM perplexity; explain what you get\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Task 3</font>: Create a unigram n-gram LM (non-uniform)\n",
    "\n",
    "Admittedly, the uniform language model is not very useful. Hence we now improve it by estimating the actual probabilities of the words in the corpus using maximum likelihood estimates. To do so, we extend the `CountLM` class (copied for convenience below) and create a `UnigramLM` class.\n",
    "\n",
    "- **Deliverable 3.1**: Complete the function `__init__` in `lm.UnigramLM` (in the `snlp/lm.py` file). It goes over each sentence in the training corpus and stores how often it sees a particular unigram. *Note*: Since we soon want to make this class more general, we will store the count of a word as tuple where the second part (its history) is for now the empty history (), hence the count of a word is stored with the following key in the default dictionary: \"(w,)\"\n",
    "- **Test**: `nosetest tests/test_lms.py:test_unigram`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(lm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CountLM(LanguageModel):\n",
    "    \"\"\"\n",
    "    A Language Model that uses counts of events \n",
    "    and histories to calculate probabilities of words in context.\n",
    "    \"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def counts(self, word_and_history):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def norm(self, history):\n",
    "        pass\n",
    "\n",
    "    def probability(self, word, *history):\n",
    "        if word not in self.vocab:\n",
    "            return 0.0\n",
    "        sub_history = tuple(history[-(self.order - 1):]) if self.order > 1 else ()\n",
    "        norm = self.norm(sub_history)\n",
    "        if norm == 0:\n",
    "            return 1.0 / len(self.vocab)\n",
    "        else:\n",
    "            return self.counts((word,) + sub_history) / self.norm(sub_history)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(lm);\n",
    "! nosetests tests/test_lms.py:test_unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Deliverable 3.2** (in the notebook): Sample a sentence from the uniform LM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Deliverable 3.3** (in the notebook): Calculate the perplexity of the dev data `corpus_dev.csv` according to the uniform and unigram LM. Which perplexity should be lower?\n",
    "- **Test**: `nosetest tests/test_lms.py:test_ppl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate perplexity of both LMs (uniform and unigram) on the dev data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nosetests tests/test_lms.py:test_ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Task 4</font>: Create a bigram n-gram language model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The language models so far ignored that certain words have a higher probability following certain other words (the conditioning history so far was empty). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Deliverable 4.1**  Implement a bigram LM. Before doing so, manually estimate the probability of all words that can follow `is` according to the corpus in `data/corpus.csv`. Then, create the `bigramLM` you just implemented to check your calculations. Finally, generate (sample) sentences from the bigram LM. Did the LM improve? How can you tell that this LM is better than the one before?\n",
    "- **Test**: `nosetest tests/test_lms.py:test_bigram`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat data/corpus.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(lm);\n",
    "! nosetests tests/test_lms.py:test_bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(sample(bigramLM, [\"<START>\"],sentence_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Task 5</font>: Out-of-Vocabulary Words - Theoretical question\n",
    "The problem in the above example is that the baseline model assigns zero probability to words that are not in the vocabulary. Test sets will usually contain such words, and this leads to the above result of infinite perplexity. For example, the following three words do not appear in the training set vocabulary `vocab` and hence receive 0 probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = preproc.create_vocab(x_train)\n",
    "unigramLM = lm.UnigramLM(vocab, x_train)\n",
    "print(unigramLM.vocab)\n",
    "\n",
    "print(unigramLM.probability(\"the\"))\n",
    "print(unigramLM.probability(\"blue\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Deliverable 5.1**  What are possible ways to deal with the problem of OOVs?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color='blue'>Task 6</font>: Language Model - Pen & Paper\n",
    "\n",
    "- **Deliverable 6.1** Solve Exercise 3.4 from J&M, copied below for convenience.\n",
    "\n",
    "```\n",
    "We are given the following corpus, modified from the one in the chapter:\n",
    "\n",
    "<s> I am Sam </s>\n",
    "<s> Sam I am </s>\n",
    "<s> I am Sam </s>\n",
    "<s> I do not like green eggs and Sam </s>\n",
    "\n",
    "Using a bigram language model with add-one smoothing, what is\n",
    "P(Sam | am)? Include <s> and </s> in your counts just like any other token.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}