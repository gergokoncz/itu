{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 - Part II: Sequence Prediction with HMMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will implement the Viterbi algorithm for decoding in sequence tagging. The parameter estimation for the parameters of the model (emission and transition) is provided, as they are similar to parameter estimation with smoothing that you have seen in the Naive Bayes lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Markov Models (HMM) for sequence tagging\n",
    "\n",
    "In this part of the exercise you are going to implement a HMM POS tagger.\n",
    "\n",
    "First, lets read in the data. We provide some helper functions for it (see `myutils.py`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import numpy as np\n",
    "import myutils\n",
    "from collections import defaultdict\n",
    "\n",
    "# load data\n",
    "train_data = myutils.read_conll_file(\"data/da_ddt-ud-train.conllu\")\n",
    "dev_data = myutils.read_conll_file(\"data/da_ddt-ud-dev.conllu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM class\n",
    "We provide an HMM class, which includes the estimation of the transition as well as the emission probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        initialize model parameters\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.START = '_START_'\n",
    "        self.UNK = 'UNK'\n",
    "        self.STOP = '_STOP_'\n",
    "        self.transitions = defaultdict(lambda: defaultdict(float))\n",
    "        self.emissions = defaultdict(lambda: defaultdict(float))\n",
    "        self.vocabulary = set()\n",
    "        self.tags = set()\n",
    "\n",
    "    def fit(self, train_data):\n",
    "        \"\"\"\n",
    "        fit model to a file in CoNLL format.\n",
    "        :param file_name:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        counts = defaultdict(int)\n",
    "\n",
    "        # record all used tags and words\n",
    "        for (words, tags) in train_data:\n",
    "            \n",
    "            # iterate over sentence\n",
    "            for i, (word, tag) in enumerate(zip(words, tags)):\n",
    "                self.tags.add(tag)\n",
    "                self.vocabulary.add(word)\n",
    "                counts[word] += 1\n",
    "\n",
    "        ## collect counts \n",
    "        for (words, tags) in train_data:\n",
    "            \n",
    "            # add stop symbol\n",
    "            words=words+[self.STOP]\n",
    "            tags=tags+[self.STOP]\n",
    "\n",
    "            # iterate over sentence\n",
    "            for i, (word, tag) in enumerate(zip(words, tags)):\n",
    "\n",
    "                if i==0:\n",
    "                    prev_tag=self.START\n",
    "\n",
    "                    # record only transition from start\n",
    "                    self.transitions[prev_tag][tag] += 1\n",
    "                    \n",
    "                else:\n",
    "                    prev_tag=tags[i-1]\n",
    "\n",
    "                    # record count for transition\n",
    "                    self.transitions[prev_tag][tag] += 1\n",
    "\n",
    "                    if i < len(words)-1:\n",
    "                        # record count for emissions\n",
    "                        if counts[word] < 2:\n",
    "                            self.emissions[tag][self.UNK] += 1\n",
    "                        # note that infrequent words are counted twice\n",
    "                        self.emissions[tag][word] += 1\n",
    "\n",
    "        ## e(tag|word) = count(t->word)/count(word)\n",
    "        for tag in self.emissions:\n",
    "            total_tag=sum(self.emissions[tag].values())\n",
    "            for word in self.emissions[tag]:\n",
    "                prob_word_given_tag = self.emissions[tag][word] / float(total_tag)\n",
    "                self.emissions[tag][word] = prob_word_given_tag\n",
    "\n",
    "        ## t(tag|prevtag) = count(prevtag,tag)/ count(prevtag)\n",
    "        for prevtag in self.transitions:\n",
    "            total_prevtag=sum(self.transitions[prevtag].values())\n",
    "            for tag in self.transitions[prevtag]:\n",
    "                prob_tag_given_prevtag = self.transitions[prevtag][tag] / float(total_prevtag)\n",
    "                self.transitions[prevtag][tag] =  prob_tag_given_prevtag\n",
    "\n",
    "    def predict(self, data, method='most_likely'):\n",
    "        \"\"\"\n",
    "        predict the most likely tag sequence for all sentences in data\n",
    "\n",
    "        :param data: a list of sentences\n",
    "        :param method: viterbi or most likely decoding\n",
    "        :return: list of predicted tag sequences\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for sentence in data:\n",
    "            if method == 'viterbi':\n",
    "                results.append(self.predict_viterbi(sentence[0]))\n",
    "            else:\n",
    "                results.append(self.predict_most_likely(sentence[0]))\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following function you are supposed to return the most likely tag for each word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_most_likely(self, sentence):\n",
    "    \"\"\"\n",
    "    predict the single most likely tag (from training data) for every token in sentence\n",
    "    (i.e., just looks at a single tag at a time, no context)\n",
    "        \n",
    "    :sentence: list of tokens\n",
    "    :return: list of tags\n",
    "    \"\"\"\n",
    "    tagSeq = []\n",
    "    for word in sentence:\n",
    "        #TODO implement, replace the string with the most likely tag for this word\n",
    "        tagSeq.append('NOUN')\n",
    "    return tagSeq\n",
    "\n",
    "# To make the function link to the class in the previous cell\n",
    "HMM.predict_most_likely = predict_most_likely\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is used to train a model, and predict using the most_likely strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most likely scores:\n",
      "sent level:  0.0124\n",
      "word level:  0.1884\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create new model\n",
    "hmm = HMM()\n",
    "\n",
    "# fit model to training data\n",
    "hmm.fit(train_data)\n",
    "\n",
    "# get most likely tag predictions \n",
    "most_likely_predictions = hmm.predict(dev_data, method='most_likely')\n",
    "\n",
    "# evaluate\n",
    "gold = [x[1] for x in dev_data]\n",
    "sent_level, word_level = myutils.evaluate(gold, most_likely_predictions)\n",
    "\n",
    "print('most likely scores:')\n",
    "print('sent level:  {:.4f}'.format(sent_level))\n",
    "print('word level:  {:.4f}'.format(word_level))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following function you are supposed to implement the Viterbi algorithm. The step from the START symbol to the first word is already implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_viterbi(self,sentence):\n",
    "    \"\"\"\n",
    "    predict the most likely tag sequences using the Viterbi algorithm\n",
    "\n",
    "    :sentence: list of tokens\n",
    "    :return: list of tags\n",
    "    \"\"\"\n",
    "\n",
    "    # replace unknown words for simplicity\n",
    "    for i in range(len(sentence)):\n",
    "        if sentence[i] not in self.vocabulary:\n",
    "            sentence[i] = self.UNK\n",
    "\n",
    "    # prepare data structures\n",
    "    N = len(sentence)\n",
    "    viterbiProbs = np.zeros((N, len(self.tags)))\n",
    "    # viterbiBacktrace can be used to remember which previous tag was used\n",
    "    viterbiBacktrace = np.zeros((N, len(self.tags)), dtype=int)\n",
    "    # make self.tags a list, so we can use indexes\n",
    "    self.tags = sorted(self.tags)\n",
    "\n",
    "    # initialize first step (from START)\n",
    "    for tagIdx, tag in enumerate(self.tags):\n",
    "        emisProb = self.emissions[tag][sentence[0]]\n",
    "        transProb = self.transitions[self.START][tag]\n",
    "        viterbiProbs[0][tagIdx] = emisProb * transProb\n",
    "\n",
    "    # process the rest of the sentence\n",
    "    for t in range(1,N):\n",
    "        #TODO implement\n",
    "        # find max probability of transition from previous states\n",
    "        # multiply with emission\n",
    "        # save both probability and path\n",
    "        pass\n",
    "\n",
    "    # final step (to STOP)\n",
    "    #TODO implement\n",
    "    \n",
    "    # retrieve the most likely sequence from backtrace\n",
    "    #TODO implement\n",
    "    \n",
    "    return NotImplementedError\n",
    "\n",
    "# To make the function link to the class in a previous cell\n",
    "HMM.predict_viterbi = predict_viterbi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code-snippet the scores of the Viterbi implementation are evaluated. How much higher is the word-level accuracy? How much higher is the sentence-level accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get viterbi predictions\n",
    "viterbi_predictions = hmm.predict(dev_data, method='viterbi')\n",
    "# evaluate\n",
    "sent_level, word_level = myutils.evaluate(gold, viterbi_predictions)\n",
    "print('viterbi scores:')\n",
    "print('sent level:  {:.4f}'.format(sent_level))\n",
    "print('word level:  {:.4f}'.format(word_level))\n",
    "print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
