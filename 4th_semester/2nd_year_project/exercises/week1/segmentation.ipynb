{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Segmentation Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color='green'>Setup 1</font>: Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Task</font> 1: Improving a sentence segmenter\n",
    "\n",
    "Sentence segmentation is not a trivial task either. There might be some cases where your simple sentence segmentation won't work properly.\n",
    "\n",
    "First, make sure you understand the following sentence segmentation code used in the lecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sentence_segment(match_regex, tokens):\n",
    "    \"\"\"\n",
    "    Splits a sequence of tokens into sentences, splitting wherever the given matching regular expression\n",
    "    matches.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    match_regex the regular expression that defines at which token to split.\n",
    "    tokens the input sequence of string tokens.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    a list of token lists, where each inner list represents a sentence.\n",
    "\n",
    "    >>> tokens = ['the','man','eats','.','She', 'sleeps', '.']\n",
    "    >>> sentence_segment(re.compile('\\.'), tokens)\n",
    "    [['the', 'man', 'eats', '.'], ['She', 'sleeps', '.']]\n",
    "    \"\"\"\n",
    "    current = []\n",
    "    sentences = [current]\n",
    "    for tok in tokens:\n",
    "        current.append(tok)\n",
    "        if match_regex.match(tok):\n",
    "            current = []\n",
    "            sentences.append(current)\n",
    "    if not sentences[-1]:\n",
    "        sentences.pop(-1)\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve the sentence segmenter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, modify the following code so that sentence segmentation returns correctly segmented sentences on the following text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['Llanfairpwllgwyngyllgogerychwyrndrobwllllantysiliogogogoch', 'is', 'the', 'longest', 'official', 'one-word', 'placename', 'in', 'U', 'K', '. ']\n[\"Isn't\", 'that', 'weird', '?']\n['I', 'mean', 'someone', 'took', 'the', 'effort', 'to', 'really', 'make', 'this', 'name', 'as', 'complicated', 'as', 'possible', 'huh', '?!']\n['Of', 'course', 'U', 'S', 'A', '. ']\n['also', 'has', 'its', 'own', 'record', 'in', 'the', 'longest', 'name', 'albeit', 'a', 'bit', 'shorter', '... ']\n['This', 'record', 'belongs', 'to', 'the', 'place', 'called', 'Chargoggagoggmanchauggagoggchaubunagungamaugg', '. ']\n[\"There's\", 'so', 'many', 'wonderful', 'little', 'details', 'one', 'can', 'find', 'out', 'while', 'browsing', 'http://www.wikipedia.org', 'during', 'their', 'Ph.D.', 'or', 'an', 'M', 'Sc', '.\\n']\n"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Llanfairpwllgwyngyllgogerychwyrndrobwllllantysiliogogogoch is the longest official one-word placename in U.K. Isn't that weird? I mean, someone took the effort to really make this name as complicated as possible, huh?! Of course, U.S.A. also has its own record in the longest name, albeit a bit shorter... This record belongs to the place called Chargoggagoggmanchauggagoggchaubunagungamaugg. There's so many wonderful little details one can find out while browsing http://www.wikipedia.org during their Ph.D. or an M.Sc.\n",
    "\"\"\"\n",
    "\n",
    "token = re.compile('https?:\\/\\/[a-z0-9.?/]+|[\\(\\)]|\\w+[\\'-]\\w+|Mr\\.|Ph\\.D\\.|[\\w\\']+|[?!]+|\\.+\\s')\n",
    "\n",
    "tokens = token.findall(text)\n",
    "sentences = sentence_segment(re.compile('\\.+\\s|[?!]'), tokens)\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "- what elements of a sentence did you have to take care of here?\n",
    "- is it useful or possible to enumerate all such possible examples?\n",
    "- how would you deal with all URLs effectively?\n",
    "- are there any specific punctuation not covered in the example you might think of?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background Reading\n",
    "\n",
    "* Jurafsky & Martin, [Speech and Language Processing (Third Edition)](https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf): Chapter 2, Regular Expressions, Text Normalization, Edit Distance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "* This notebook is adapted from the course material which originates from Sebastian Riedel https://github.com/uclnlp/stat-nlp-book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}