{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What do word embeddings represent?\n",
    "In this exercise, you are going to explore what is represented in word embeddings. We are going to make use of the python gensim package and two sets of pre-trained embeddings. The embeddings can be downloaded from:\n",
    "\n",
    "* www.robvandergoot.com/data/embeds/twitter.bin.gz\n",
    "* www.robvandergoot.com/data/embeds/googlenews.bin.gz\n",
    "\n",
    "The first embeddings are skip-gram embeddings trained on a collection of 2 billion words from English tweets collected during 2012 and 2018 with the default settings of word2vec. The second embeddings are trained on 100 billion words from Google News. They have both been truncated to the most frequent 500,000 words. Note that loading that each of these embeddings require approximately 2GB of ram.\n",
    "\n",
    "The embeddings can be loaded in gensim as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading finished\n"
     ]
    }
   ],
   "source": [
    "import gensim.models\n",
    "\n",
    "twitEmbs = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "                                'twitter.bin', binary=True)\n",
    "print('loading finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now use the index operator ``[]`` or the function ``get_vector()`` to acces the individual word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.64285821e-01,  2.37979457e-01, -4.24226150e-02, -4.35831666e-01,\n",
       "       -4.06450212e-01, -1.43117514e-02,  1.22334510e-01, -5.59092343e-01,\n",
       "        1.23332568e-01,  2.36625358e-01,  3.58797014e-02, -9.40739065e-02,\n",
       "       -2.04128489e-01, -1.81295779e-02, -1.08792759e-01, -2.70818472e-01,\n",
       "        1.05479717e-01,  1.37095019e-01,  1.79271579e-01,  2.91243941e-01,\n",
       "       -5.87746739e-01,  2.90462654e-02,  6.89281642e-01, -1.80917114e-01,\n",
       "       -2.57750720e-01, -2.01395631e-01, -5.16403615e-01,  5.85804135e-03,\n",
       "       -1.67768478e-01,  2.17095211e-01,  2.22494245e-01,  1.56742647e-01,\n",
       "       -3.60864878e-01,  3.94283593e-01,  8.04448500e-03,  1.11518592e-01,\n",
       "       -1.85592070e-01, -1.16088443e-01,  3.24357510e-01,  4.00876179e-02,\n",
       "        9.14092362e-02, -1.04118213e-01, -6.89513862e-01,  1.54412836e-01,\n",
       "        4.57625002e-01,  2.55037360e-02, -3.84058757e-03,  7.12698698e-02,\n",
       "       -2.25590184e-01, -1.96693689e-01, -3.88458431e-01, -2.27625713e-01,\n",
       "        6.94357634e-01, -3.22451681e-01,  1.02136515e-01, -2.06018016e-01,\n",
       "        4.12042558e-01, -5.69718063e-01, -1.77221447e-01, -7.04838037e-01,\n",
       "        5.86289287e-01,  1.18259907e-01, -5.15342169e-02,  3.12465429e-01,\n",
       "       -5.25288224e-01,  5.48078716e-01,  2.75395304e-01, -1.61753371e-01,\n",
       "        4.37383980e-01, -9.72139016e-02, -1.71533942e-01,  3.94486845e-01,\n",
       "        1.33596465e-01,  3.94779667e-02,  1.23597078e-01,  3.22522134e-01,\n",
       "       -1.40469015e-01, -7.82357603e-02, -3.39861751e-01, -4.84348953e-01,\n",
       "        8.03721175e-02,  1.13537483e-01, -6.08491674e-02,  2.59142101e-01,\n",
       "        3.79286081e-01, -3.21717769e-01,  3.45237699e-04,  4.53131020e-01,\n",
       "        2.97795296e-01,  4.74226564e-01, -4.53676343e-01,  3.24836336e-02,\n",
       "       -3.32390517e-01, -2.07979798e-01, -1.37789533e-01, -1.56768903e-01,\n",
       "       -4.80998158e-02,  1.80168718e-01,  6.78501278e-03, -9.98419821e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitEmbs['cat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word similarities\n",
    "Cosine distance can be used to measure the distance between two words; it is defined as:\n",
    "\\begin{equation}\n",
    "cos_{\\vec{a},\\vec{b}} = \\frac{\\vec{a} \\cdot \\vec{b}}{|\\vec{a}| |\\vec{b}|} = \\frac{\\sum^n_1 a_i b_i}{\\sqrt{\\sum^n_1 a_i^2} \\sqrt{\\sum^n_1 b_i^2}}\n",
    "\\end{equation}\n",
    "\n",
    "**1. Implement the cosine similarity using pure python (only the ``math`` package is allowed).** Note that similarity is 1-distance.\n",
    "\n",
    "You can compare your scores to the gensim implementation to check wheter it is correct. The following code should give the same output\n",
    "\n",
    "```\n",
    "print(twitEmbs.distance('cat', 'dog'))\n",
    "print(cosine(twitEmbs['cat'], twitEmbs['dog']))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In wordnet, the distance between two senses can be based on the distance in the taxonomy, the most common metric for this is:\n",
    "\n",
    "Wu-Palmer Similarity: denotes how similar two word senses are, based on the depth of the two senses in the taxonomy and that of their Least Common Subsumer (most specific ancestor node).\n",
    "\n",
    "It can be computed in python like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordNet similarity: 0.8571428571428571\n",
      "Twitter similarity: 0.8955348\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "first_word = wordnet.synsets('cat')[0] #0 means: most common sense\n",
    "second_word = wordnet.synsets('dog')[0]\n",
    "print('WordNet similarity: ' + str(first_word.wup_similarity(second_word)))\n",
    "\n",
    "print('Twitter similarity: ' + str(twitEmbs.similarity('cat', 'dog')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**2. Think of 5 word pairs which have a high similarity according to you. Estimate the difference between these pairs in wordnet as well as in the Twitter embeddings and the Googlenews embeddings. Which method is closest to your own intuition?** (you are allowed to use the gensim implementation of cosine similarity here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analogies\n",
    "\n",
    "Analogies have often been used to demonstrate the power of word embeddings. Analogies have the form ``A :: B : C :: D``. In this setting A, B and C are usually given and the fourth term is extracted from the embeddings by using ``3cosadd``:\n",
    "\n",
    "\\begin{equation}\n",
    "\\underset{d}{\\mathrm{argmax}} (\\cos (d, c) - \\cos (d, a) + \\cos (d, b))\n",
    "\\label{equ:cosadd}\n",
    "\\end{equation}\n",
    "\n",
    "You can query analogies with gensim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.8401797413825989),\n",
       " ('goddess', 0.7309160828590393),\n",
       " ('king…', 0.7233694195747375),\n",
       " ('princess', 0.715788722038269),\n",
       " ('kings', 0.707615852355957),\n",
       " ('godess', 0.6952610015869141),\n",
       " ('Queen', 0.6902579069137573),\n",
       " ('queen,', 0.6876209378242493),\n",
       " ('quee…', 0.6856900453567505),\n",
       " ('queens', 0.6832401156425476)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitEmbs.most_similar(positive=['woman', 'king'], negative=['man'], \n",
    "                                                         topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``3cosadd`` can be used to solve semantic as well as syntactic analogies:\n",
    "\n",
    "| Syntactic           |                                      |\n",
    "|---------------------|--------------------------------------|\n",
    "| Country-capital     | Denmark :: Copenhagen : England :: X |\n",
    "| Family-relations    | boy :: girl : he :: X                |\n",
    "| Object-color        | sky :: blue : grass :: X             |\n",
    "\n",
    "| Semantic            |                                      |\n",
    "|---------------------|--------------------------------------|\n",
    "| Superlatives        | nice :: nicer : good :: X            |\n",
    "| Present-past tense  | work :: worked : drink :: X          |\n",
    "| Country-nationality | Brazil :: Brazilian : Denmark :: X   |\n",
    "\n",
    "\n",
    "Try the analogies from the table. Is the correct answer returned for all queries?, if not; are the answers at least ranked high?\n",
    "\n",
    "**1. Think of another category of semantic analogies that might be encoded in the embeddings and test this empirically by thinking of 5 example analogies. Which embeddings are better at predicting your category (Twitter versus news)?**\n",
    "\n",
    "**2. Think of another category of syntactic analogies that might be encoded in the embeddings and test this empirically by thinking of 5 example analogies. Which embeddings are better at predicting your category (Twitter versus news)?**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
