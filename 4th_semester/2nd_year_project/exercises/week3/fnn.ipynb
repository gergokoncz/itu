{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 - A Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this exercise, you will implement the forward step of a FFNN from scratch and compare your solution to Keras on a small toy example to predict the nationality for a given surname. \n",
    "\n",
    "It is very important that you understand the basic building blocks (input/output: how to encode your instances, the labels; the model: what the neural network consists of, how to learn its weights, how to do a forward pass for prediction). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward Neural Networks (FNNs) or MLPs\n",
    "\n",
    "Feedforward Neural Networks (FNNs) are also called Multilayer Perceptrons (MLPs). These are the most basic types of neural networks. They are called this way as the information is flowing from the input nodes through the network up to the output nodes. \n",
    "\n",
    "It is essential to understand that a neural network is a non-linear classification model which is based upon function application. Each layer in a neural network is an application of a function.\n",
    "\n",
    "Summary (by J.Frellsen):\n",
    "<img src=\"pics/fnn_jf.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper functions\n",
    "\n",
    "## use this softmax\n",
    "from keras import backend as K\n",
    "from keras  import activations\n",
    "\n",
    "def keras_softmax(scores):\n",
    "    ## softmax calculation\n",
    "    var = K.variable(value=scores)\n",
    "    act_tf = activations.softmax(var) # returns Tensor\n",
    "    softmax_scores = K.eval(act_tf) # return numpy array\n",
    "    return softmax_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surname Nationality Prediction with a FNN\n",
    "\n",
    "What you will learn:\n",
    "\n",
    "* how to encode the data as character-based features and feed this n-hot representation as input to a FNN\n",
    "* how to define the model (FNN) by reading off its structure from a graphical illustration of the network \n",
    "* compute the forward pass manually by loading existing weights for the model; to know whether your implementation is correct, you will compare the computed prediction scores to a model implemented in Keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color='blue'>Task 1</font>: Representing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are assuming multi-class classification tasks. The labels are $$ y \\in \\{da,no,se\\}$$\n",
    "\n",
    "The data comes from the national statistics banks. We here consider a very small dataset. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training data\n",
    "import numpy as np\n",
    "\n",
    "data = [('da','Nielsen'), # Statistik Danmark: https://www.dst.dk/en/Statistik/emner/befolkning-og-valg/navne/navne-i-hele-befolkningen\n",
    "        ('da','Jensen'),\n",
    "        ('da','Hansen'),\n",
    "        ('da','Pedersen'),\n",
    "        ('da','Andersen'),\n",
    "        ('se','Andersson'), # Statistics Sweden: https://www.scb.se/en/finding-statistics/statistics-by-subject-area/population/general-statistics/name-statistics/pong/tables-and-graphs/all-registered-persons-in-sweden---last-names-top-100-list/last-names-top-100/\n",
    "        ('se','Johansson'),\n",
    "        ('se','Karlsson'),\n",
    "        ('se','Nilsson'),\n",
    "        ('se','Eriksson'),\n",
    "        ('no','Hansen'), # Statistics Norway: https://www.ssb.no/en/befolkning/statistikker/navn/aar/2015-01-27?fane=tabell&sort=nummer&tabell=216083\n",
    "        ('no','Johansen'),\n",
    "        ('no','Olsen'),\n",
    "        ('no','Larsen'),\n",
    "        ('no','Andersen'),\n",
    "       ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **task**: Convert the data into n-hot format, where each feature represents whether a single character is present or not.  Similarly, convert the labels into numeric format. For simplicity, you can assume a closed vocabulary (only the letters that you see above, no unknown-word handling). Keep original casing.\n",
    "  * What is the vocabulary size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "## character to index, label to index\n",
    "char2idx = defaultdict(int)\n",
    "label2idx = defaultdict(int)\n",
    "\n",
    "## Your code here\n",
    "# Hint: fill in this structure data_train = np.zeros((len(data),vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color='blue'>Task 1</font>: Forward pass (from scratch)\n",
    "\n",
    "You are going to implement the forward step manually on a small dataset. \n",
    "\n",
    "Implement the forward pass for the feedforward neural network illustrated in the figure by solely using `numpy`.\n",
    "\n",
    "* How many neurons do hidden layer 1 and hidden layer 2 have? Note: the bias node is not shown in the figure, consider them as separate neurons.\n",
    "* How many neurons does the output layer have? And the input layer? (Note: the figure shows only 4 input nodes, in this example your input size is defined above - what is the input layer size?)\n",
    "* Assume there is a `tanh` activation function between the layers. (hint: you can use `np.tanh`)\n",
    "* Which activation function is on the output layer, given the labels above?\n",
    "* Hint: use `.shape` to check the dimensions of your inputs\n",
    "\n",
    "<img src=\"pics/nn.svg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the size of layers of the feedforward neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper functions to determine the input and output dimensions of each layer\n",
    "# input_dim = ..\n",
    "#hidden_dim1 = ..\n",
    "# etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the shape of the parameters to be learned for this network using numpy arrays. For now, simply initialize them arbitrarily. You can use ones or random numbers, e.g., `np.ones((3,4))` defines a matrix of ones of size `3x4`, similarly, [np.random.randn](https://www.numpy.org/devdocs/reference/generated/numpy.random.randn.html) `np.random.randn(3,4)` initializes a matrix of the same size with random sample from the standard normal distribution.\n",
    "\n",
    "* What are all the parameters of this neural network and what is their shape?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define all parameters of this NN (W_1, ... bias1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the shape of all parameters, we are ready to \"connect the dots\" and build the network. \n",
    "\n",
    "It is instructive to break the computation of each layer down into two steps: the scores $a1$ are obtained by the linear function followed by the activation applications $\\sigma$ to obtain the representation $z1$, as in:\n",
    "\n",
    "$$ a1 = xW_1 + b_1$$\n",
    "$$ z1 = \\sigma(a1)$$\n",
    "\n",
    "Specify the entire network up to the output layer $z3$, and **up to and exclusive** the final application of the softmax, the last activation function, which is provided.\n",
    "\n",
    "The exact implementation of the softmax might differ from toolkit to toolkit (due to variations in implementation details in order to obtain numerical stability). Therefore, we will use the Keras backend function for the softmax calculation which accesses the tensorflow `Tensor` object. This makes sure that the manual calculations of the forward pass due not differ from the Keras-based implementation just because of the difference in the softmax calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## implement the forward pass (up to and exclusive the softmax) \n",
    "## apply it to the training data `data_train` - use vectorization\n",
    "\n",
    "#final_scores = None\n",
    "#y_hat_manual = keras_softmax(final_scores)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the resulting predictions will be the softmax activations for each output neuron for each training instance\n",
    "print(y_hat_manual.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_hat_manual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that all predictions sum up to approximately 1 (hint: use `np.sum` with `axis`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y_hat_manual, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Congrats! you have made it through the manual construction of the forward pass. Now lets check your implementation by comparing it to a set of pre-determined weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color='blue'>Task 2</font>: Where do the weights come from?  Loading existing weights\n",
    "\n",
    "So far, the model that you used randomly initialized weights. In this step we will load pre-trained model weights and do the forward pass with those weights, in order to check your implementation against model predictions computed by the toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to:\n",
    "* load pretrained weights for all parameters\n",
    "* apply the weights to the evaluation data `data_eval`\n",
    "* check that your manual softmax scores match the ones obtained by the pre-trained model `model` that we will load\n",
    "* convert the output to labels and calculate the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data/weights.pickle\",\"rb\") as f:\n",
    "    weights = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the weights you just loaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## inspect the weights to get familiar with the structure of the loaded model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply your manual implementation of the forward pass to the evaluation data by using the parameters (weights) you just loaded. This allows you to check if you get the same results back as the model implemented in Keras. \n",
    "\n",
    "* **Task**: Convert the following test data into the input format for the neural network above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create data_eval matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is the build-in forward pass and what we would like to get as well\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('data/model.h5') # load model parameters and model structure\n",
    "\n",
    "# use the model for predicting on the data_eval\n",
    "predictions = model.predict(data_eval)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* **Task**: Now use the weights stored in  `weights` to your manually defined forward pass above. Compare the result to the predictions of the loaded model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weights and code up the forward pass manually. Compare to the predictions above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the two softmax outputs match, your implementation is correct. Congrats!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Task**: Finally, get the labels back. Convert the output from the softmax into predicted labels. What do you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(optional) In reality, you will train the model on data to estimate its weight. If you like, train the model on the data above with SGD and 5 epochs. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
